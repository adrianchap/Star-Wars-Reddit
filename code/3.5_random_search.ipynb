{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: this notebook contains experiments with Random Search CV - which I did not end up using for my presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import train_test_split. Crossval score. Gridsearch CV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "# Import Standard Scaller\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Import logistic regression. KNN.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Import metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "#Import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "# imports\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "\n",
    "# Import CountVectorizer and TFIDFVectorizer from feature_extraction.text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/text_count_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split into training & testing sets\n",
    "X = df['features']\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.501002\n",
       "0    0.498998\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1494,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cvec__max_features': [2000, 3000, 4000, 5000], 'cvec__min_df': [2, 3], 'cvec__max_df': [0.9, 0.95], 'cvec__ngram_range': [(1, 1), (1, 2)], 'rf__n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'rf__max_features': ['auto', 'sqrt'], 'rf__max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'rf__min_samples_split': [2, 5, 10], 'rf__min_samples_leaf': [1, 2, 4], 'rf__bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'cvec__max_features':[2000, 3000, 4000, 5000],\n",
    "               'cvec__min_df' : [2, 3],\n",
    "               'cvec__max_df': [.9, .95],\n",
    "               'cvec__ngram_range': [(1, 1), (1, 2)],\n",
    "               'rf__n_estimators': n_estimators,\n",
    "               'rf__max_features': max_features,\n",
    "               'rf__max_depth': max_depth,\n",
    "               'rf__min_samples_split': min_samples_split,\n",
    "               'rf__min_samples_leaf': min_samples_leaf,\n",
    "               'rf__bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  3.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                             ('rf', RandomForestClassifier())]),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'cvec__max_df': [0.9, 0.95],\n",
       "                                        'cvec__max_features': [2000, 3000, 4000,\n",
       "                                                               5000],\n",
       "                                        'cvec__min_df': [2, 3],\n",
       "                                        'cvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                                        'rf__bootstrap': [True, False],\n",
       "                                        'rf__max_depth': [10, 20, 30, 40, 50,\n",
       "                                                          60, 70, 80, 90, 100,\n",
       "                                                          110, None],\n",
       "                                        'rf__max_features': ['auto', 'sqrt'],\n",
       "                                        'rf__min_samples_leaf': [1, 2, 4],\n",
       "                                        'rf__min_samples_split': [2, 5, 10],\n",
       "                                        'rf__n_estimators': [200, 400, 600, 800,\n",
       "                                                             1000, 1200, 1400,\n",
       "                                                             1600, 1800,\n",
       "                                                             2000]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(pipe, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf__n_estimators': 1000,\n",
       " 'rf__min_samples_split': 10,\n",
       " 'rf__min_samples_leaf': 2,\n",
       " 'rf__max_features': 'auto',\n",
       " 'rf__max_depth': 40,\n",
       " 'rf__bootstrap': False,\n",
       " 'cvec__ngram_range': (1, 1),\n",
       " 'cvec__min_df': 3,\n",
       " 'cvec__max_features': 2000,\n",
       " 'cvec__max_df': 0.9}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy = 0.9223560910307899\n",
      "Test accuracy = 0.7615230460921844\n",
      "Specificity = 0.7389558232931727\n",
      "Recall = 0.784\n"
     ]
    }
   ],
   "source": [
    "# Score model on training set.\n",
    "print(f'Train accuracy = {rf_random.score(X_train, y_train)}')\n",
    "# Score model on testing set.\n",
    "print(f'Test accuracy = {rf_random.score(X_test, y_test)}')\n",
    "# Get predictions\n",
    "preds = rf_random.predict(X_test)\n",
    "# Save confusion matrix values\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "# Calculate the specificity\n",
    "print(f'Specificity = {tn / (tn + fp)}')\n",
    "# Calculate the recall\n",
    "print(f'Recall = {tp / (tp + fn)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the transformer.\n",
    "tvec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split into training & testing sets\n",
    "X = df['features']\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tvec__max_features': [2000, 3000, 4000, 5000], 'tvec__stop_words': [None, 'english'], 'tvec__min_df': [2, 3], 'tvec__max_df': [0.9, 0.95], 'tvec__ngram_range': [(1, 1), (1, 2)], 'rf__n_estimators': [50, 100, 150, 200, 250, 300, 350, 400, 450, 500], 'rf__max_features': ['auto', 'sqrt'], 'rf__max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'rf__min_samples_split': [2, 5, 10], 'rf__min_samples_leaf': [1, 2, 4], 'rf__bootstrap': [True, False]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = { 'tvec__max_features': [2000, 3000, 4000, 5000],\n",
    "               'tvec__stop_words': [None, 'english'],\n",
    "               'tvec__min_df' : [2, 3],\n",
    "               'tvec__max_df': [.9, .95],\n",
    "               'tvec__ngram_range': [(1,1), (1,2)],\n",
    "               'rf__n_estimators': n_estimators,\n",
    "               'rf__max_features': max_features,\n",
    "               'rf__max_depth': max_depth,\n",
    "               'rf__min_samples_split': min_samples_split,\n",
    "               'rf__min_samples_leaf': min_samples_leaf,\n",
    "               'rf__bootstrap': bootstrap}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a pipeline up with two stages:\n",
    "# 1. tf-idf vectorizer (transformer)\n",
    "# 2. Multinomial Naive Bayes (estimator)\n",
    "pipe_tvec = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed:   19.9s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:   41.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                             ('rf', RandomForestClassifier())]),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'rf__bootstrap': [True, False],\n",
       "                                        'rf__max_depth': [10, 20, 30, 40, 50,\n",
       "                                                          60, 70, 80, 90, 100,\n",
       "                                                          110, None],\n",
       "                                        'rf__max_features': ['auto', 'sqrt'],\n",
       "                                        'rf__min_samples_leaf': [1, 2, 4],\n",
       "                                        'rf__min_samples_split': [2, 5, 10],\n",
       "                                        'rf__n_estimators': [50, 100, 150, 200,\n",
       "                                                             250, 300, 350, 400,\n",
       "                                                             450, 500],\n",
       "                                        'tvec__max_df': [0.9, 0.95],\n",
       "                                        'tvec__max_features': [2000, 3000, 4000,\n",
       "                                                               5000],\n",
       "                                        'tvec__min_df': [2, 3],\n",
       "                                        'tvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                                        'tvec__stop_words': [None, 'english']},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(pipe_tvec, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tvec__stop_words': 'english',\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__min_df': 3,\n",
       " 'tvec__max_features': 5000,\n",
       " 'tvec__max_df': 0.95,\n",
       " 'rf__n_estimators': 350,\n",
       " 'rf__min_samples_split': 2,\n",
       " 'rf__min_samples_leaf': 4,\n",
       " 'rf__max_features': 'auto',\n",
       " 'rf__max_depth': 80,\n",
       " 'rf__bootstrap': False}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score model on training set.\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy = 0.8775100401606426\n",
      "Test accuracy = 0.7274549098196392\n",
      "Specificity = 0.7450199203187251\n",
      "Recall = 0.7096774193548387\n"
     ]
    }
   ],
   "source": [
    "# Score model on training set.\n",
    "print(f'Train accuracy = {rf_random.score(X_train, y_train)}')\n",
    "# Score model on testing set.\n",
    "print(f'Test accuracy = {rf_random.score(X_test, y_test)}')\n",
    "# Get predictions\n",
    "preds = rf_random.predict(X_test)\n",
    "# Save confusion matrix values\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "# Calculate the specificity\n",
    "print(f'Specificity = {tn / (tn + fp)}')\n",
    "# Calculate the recall\n",
    "print(f'Recall = {tp / (tp + fn)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tvec__max_features': [2000, 3000, 4000, 5000], 'tvec__stop_words': [None, 'english'], 'tvec__min_df': [2, 3], 'tvec__max_df': [0.9, 0.95], 'tvec__ngram_range': [(1, 1), (1, 2)], 'gb__learning_rate': [0.15, 0.1, 0.05, 0.01, 0.005, 0.001], 'gb__n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'gb__max_features': ['auto', 'sqrt'], 'gb__max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'gb__min_samples_split': [2, 5, 10], 'gb__min_samples_leaf': [1, 2, 4]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "learning_rate = [0.15,0.1,0.05,0.01,0.005,0.001]\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'tvec__max_features': [2000, 3000, 4000, 5000],\n",
    "               'tvec__stop_words': [None, 'english'],\n",
    "               'tvec__min_df' : [2, 3],\n",
    "               'tvec__max_df': [.9, .95],\n",
    "               'tvec__ngram_range': [(1,1), (1,2)],\n",
    "               'gb__learning_rate': learning_rate,\n",
    "               'gb__n_estimators': n_estimators,\n",
    "               'gb__max_features': max_features,\n",
    "               'gb__max_depth': max_depth,\n",
    "               'gb__min_samples_split': min_samples_split,\n",
    "               'gb__min_samples_leaf': min_samples_leaf}\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "# Let's set a pipeline up with two stages:\n",
    "# 1. tf-idf vectorizer (transformer)\n",
    "# 2. Multinomial Naive Bayes (estimator)\n",
    "pipe_tvec = Pipeline([\n",
    "    ('tvec', TfidfVectorizer()),\n",
    "    ('gb', GradientBoostingClassifier())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   38.0s\n",
      "[Parallel(n_jobs=-1)]: Done 130 tasks      | elapsed: 23.1min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 43.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('tvec', TfidfVectorizer()),\n",
       "                                             ('gb',\n",
       "                                              GradientBoostingClassifier())]),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'gb__learning_rate': [0.15, 0.1, 0.05,\n",
       "                                                              0.01, 0.005,\n",
       "                                                              0.001],\n",
       "                                        'gb__max_depth': [10, 20, 30, 40, 50,\n",
       "                                                          60, 70, 80, 90, 100,\n",
       "                                                          110, None],\n",
       "                                        'gb__max_features': ['auto', 'sqrt'],\n",
       "                                        'gb__min_samples_leaf': [1, 2, 4],\n",
       "                                        'gb__min_samples_split': [2, 5, 10],\n",
       "                                        'gb__n_estimators': [200, 400, 600, 800,\n",
       "                                                             1000, 1200, 1400,\n",
       "                                                             1600, 1800, 2000],\n",
       "                                        'tvec__max_df': [0.9, 0.95],\n",
       "                                        'tvec__max_features': [2000, 3000, 4000,\n",
       "                                                               5000],\n",
       "                                        'tvec__min_df': [2, 3],\n",
       "                                        'tvec__ngram_range': [(1, 1), (1, 2)],\n",
       "                                        'tvec__stop_words': [None, 'english']},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "#rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "gb_random = RandomizedSearchCV(pipe_tvec, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "gb_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tvec__stop_words': 'english',\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__min_df': 2,\n",
       " 'tvec__max_features': 4000,\n",
       " 'tvec__max_df': 0.95,\n",
       " 'gb__n_estimators': 600,\n",
       " 'gb__min_samples_split': 10,\n",
       " 'gb__min_samples_leaf': 4,\n",
       " 'gb__max_features': 'sqrt',\n",
       " 'gb__max_depth': 40,\n",
       " 'gb__learning_rate': 0.01}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score model on training set.\n",
    "gb_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ccp_alpha', 'criterion', 'init', 'learning_rate', 'loss', 'max_depth', 'max_features', 'max_leaf_nodes', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_iter_no_change', 'presort', 'random_state', 'subsample', 'tol', 'validation_fraction', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier()\n",
    "gb.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.964524765729585\n",
      "Test score:  0.7234468937875751\n",
      "Specificity = 0.7290836653386454\n",
      "Recall = 0.717741935483871\n"
     ]
    }
   ],
   "source": [
    "print('Train score: ', gb_random.score(X_train, y_train))\n",
    "print('Test score: ', gb_random.score(X_test, y_test))\n",
    "preds = gb_random.predict(X_test)\n",
    "# Save confusion matrix values\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "# Calculate the specificity\n",
    "print(f'Specificity = {tn / (tn + fp)}')\n",
    "# Calculate the recall\n",
    "print(f'Recall = {tp / (tp + fn)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
